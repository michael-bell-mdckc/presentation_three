{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>otu_id</th>\n",
       "      <th>otu_label</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>...</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AXP</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>-0.339676</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.359279</td>\n",
       "      <td>0.171095</td>\n",
       "      <td>0.047010</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>-0.143251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070672</td>\n",
       "      <td>0.106434</td>\n",
       "      <td>0.206819</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>-0.243239</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>-0.030422</td>\n",
       "      <td>0.304835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.163384</td>\n",
       "      <td>-0.099845</td>\n",
       "      <td>0.851785</td>\n",
       "      <td>0.195363</td>\n",
       "      <td>0.224479</td>\n",
       "      <td>0.076721</td>\n",
       "      <td>0.197358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666731</td>\n",
       "      <td>-0.022303</td>\n",
       "      <td>-0.007904</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>0.039185</td>\n",
       "      <td>-0.231932</td>\n",
       "      <td>0.444654</td>\n",
       "      <td>0.726567</td>\n",
       "      <td>-0.179165</td>\n",
       "      <td>0.204403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DIS</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.248466</td>\n",
       "      <td>-0.209897</td>\n",
       "      <td>0.401704</td>\n",
       "      <td>0.205888</td>\n",
       "      <td>-0.128790</td>\n",
       "      <td>0.435979</td>\n",
       "      <td>-0.046332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175764</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.331345</td>\n",
       "      <td>0.522512</td>\n",
       "      <td>0.254216</td>\n",
       "      <td>0.120639</td>\n",
       "      <td>0.025639</td>\n",
       "      <td>0.036984</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.346565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HD</td>\n",
       "      <td>-0.331899</td>\n",
       "      <td>0.120603</td>\n",
       "      <td>-0.526192</td>\n",
       "      <td>0.478752</td>\n",
       "      <td>0.205863</td>\n",
       "      <td>-0.049030</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>-0.330379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240235</td>\n",
       "      <td>0.229130</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>0.322462</td>\n",
       "      <td>0.306183</td>\n",
       "      <td>0.283122</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>0.435203</td>\n",
       "      <td>-0.076475</td>\n",
       "      <td>0.427053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IBM</td>\n",
       "      <td>-0.240567</td>\n",
       "      <td>0.438470</td>\n",
       "      <td>-0.352779</td>\n",
       "      <td>0.185081</td>\n",
       "      <td>0.069837</td>\n",
       "      <td>-0.161632</td>\n",
       "      <td>0.194059</td>\n",
       "      <td>0.128171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140393</td>\n",
       "      <td>0.270309</td>\n",
       "      <td>0.043179</td>\n",
       "      <td>-0.014898</td>\n",
       "      <td>-0.122261</td>\n",
       "      <td>-0.118897</td>\n",
       "      <td>0.270549</td>\n",
       "      <td>-0.045640</td>\n",
       "      <td>-0.230799</td>\n",
       "      <td>0.257397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   otu_id otu_label      2000      2001      2002      2003      2004  \\\n",
       "0       1       AXP  0.006869 -0.339676  0.000563  0.359279  0.171095   \n",
       "1       2       CAT  0.002751  0.163384 -0.099845  0.851785  0.195363   \n",
       "2       3       DIS  0.000113 -0.248466 -0.209897  0.401704  0.205888   \n",
       "3       4        HD -0.331899  0.120603 -0.526192  0.478752  0.205863   \n",
       "4       5       IBM -0.240567  0.438470 -0.352779  0.185081  0.069837   \n",
       "\n",
       "       2005      2006      2007  ...      2010      2011      2012      2013  \\\n",
       "0  0.047010  0.185383 -0.143251  ...  0.070672  0.106434  0.206819  0.569791   \n",
       "1  0.224479  0.076721  0.197358  ...  0.666731 -0.022303 -0.007904 -0.002906   \n",
       "2 -0.128790  0.435979 -0.046332  ...  0.175764  0.010597  0.331345  0.522512   \n",
       "3 -0.049030  0.012380 -0.330379  ...  0.240235  0.229130  0.490326  0.322462   \n",
       "4 -0.161632  0.194059  0.128171  ...  0.140393  0.270309  0.043179 -0.014898   \n",
       "\n",
       "       2014      2015      2016      2017      2018      2019  \n",
       "0  0.034735 -0.243239  0.108990  0.347316 -0.030422  0.304835  \n",
       "1  0.039185 -0.231932  0.444654  0.726567 -0.179165  0.204403  \n",
       "2  0.254216  0.120639  0.025639  0.036984  0.022466  0.346565  \n",
       "3  0.306183  0.283122  0.052467  0.435203 -0.076475  0.427053  \n",
       "4 -0.122261 -0.118897  0.270549 -0.045640 -0.230799  0.257397  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file into a pandas DataFrame\n",
    "\n",
    "performance = pd.read_csv('../data/samples.csv')\n",
    "performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006869</td>\n",
       "      <td>-0.339676</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.359279</td>\n",
       "      <td>0.171095</td>\n",
       "      <td>0.047010</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>-0.143251</td>\n",
       "      <td>-0.638984</td>\n",
       "      <td>1.258279</td>\n",
       "      <td>0.070672</td>\n",
       "      <td>0.106434</td>\n",
       "      <td>0.206819</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>-0.243239</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>-0.030422</td>\n",
       "      <td>0.304835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.163384</td>\n",
       "      <td>-0.099845</td>\n",
       "      <td>0.851785</td>\n",
       "      <td>0.195363</td>\n",
       "      <td>0.224479</td>\n",
       "      <td>0.076721</td>\n",
       "      <td>0.197358</td>\n",
       "      <td>-0.368758</td>\n",
       "      <td>0.324359</td>\n",
       "      <td>0.666731</td>\n",
       "      <td>-0.022303</td>\n",
       "      <td>-0.007904</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>0.039185</td>\n",
       "      <td>-0.231932</td>\n",
       "      <td>0.444654</td>\n",
       "      <td>0.726567</td>\n",
       "      <td>-0.179165</td>\n",
       "      <td>0.204403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.248466</td>\n",
       "      <td>-0.209897</td>\n",
       "      <td>0.401704</td>\n",
       "      <td>0.205888</td>\n",
       "      <td>-0.128790</td>\n",
       "      <td>0.435979</td>\n",
       "      <td>-0.046332</td>\n",
       "      <td>-0.286749</td>\n",
       "      <td>0.432804</td>\n",
       "      <td>0.175764</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.331345</td>\n",
       "      <td>0.522512</td>\n",
       "      <td>0.254216</td>\n",
       "      <td>0.120639</td>\n",
       "      <td>0.025639</td>\n",
       "      <td>0.036984</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.346565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.331899</td>\n",
       "      <td>0.120603</td>\n",
       "      <td>-0.526192</td>\n",
       "      <td>0.478752</td>\n",
       "      <td>0.205863</td>\n",
       "      <td>-0.049030</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>-0.330379</td>\n",
       "      <td>-0.116666</td>\n",
       "      <td>0.301407</td>\n",
       "      <td>0.240235</td>\n",
       "      <td>0.229130</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>0.322462</td>\n",
       "      <td>0.306183</td>\n",
       "      <td>0.283122</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>0.435203</td>\n",
       "      <td>-0.076475</td>\n",
       "      <td>0.427053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.240567</td>\n",
       "      <td>0.438470</td>\n",
       "      <td>-0.352779</td>\n",
       "      <td>0.185081</td>\n",
       "      <td>0.069837</td>\n",
       "      <td>-0.161632</td>\n",
       "      <td>0.194059</td>\n",
       "      <td>0.128171</td>\n",
       "      <td>-0.214115</td>\n",
       "      <td>0.591181</td>\n",
       "      <td>0.140393</td>\n",
       "      <td>0.270309</td>\n",
       "      <td>0.043179</td>\n",
       "      <td>-0.014898</td>\n",
       "      <td>-0.122261</td>\n",
       "      <td>-0.118897</td>\n",
       "      <td>0.270549</td>\n",
       "      <td>-0.045640</td>\n",
       "      <td>-0.230799</td>\n",
       "      <td>0.257397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.277085</td>\n",
       "      <td>0.027439</td>\n",
       "      <td>-0.510107</td>\n",
       "      <td>1.008141</td>\n",
       "      <td>-0.272635</td>\n",
       "      <td>0.069775</td>\n",
       "      <td>-0.179676</td>\n",
       "      <td>0.329087</td>\n",
       "      <td>-0.426530</td>\n",
       "      <td>0.436096</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.194452</td>\n",
       "      <td>-0.133559</td>\n",
       "      <td>0.275995</td>\n",
       "      <td>0.452810</td>\n",
       "      <td>-0.032263</td>\n",
       "      <td>0.106119</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>0.037185</td>\n",
       "      <td>0.292457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.141905</td>\n",
       "      <td>0.151746</td>\n",
       "      <td>-0.075660</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>0.251665</td>\n",
       "      <td>-0.032509</td>\n",
       "      <td>0.106585</td>\n",
       "      <td>0.034375</td>\n",
       "      <td>-0.075747</td>\n",
       "      <td>0.107210</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>0.085230</td>\n",
       "      <td>0.108091</td>\n",
       "      <td>0.328713</td>\n",
       "      <td>0.179146</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.164676</td>\n",
       "      <td>0.237922</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.075296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.044133</td>\n",
       "      <td>-0.158802</td>\n",
       "      <td>-0.316161</td>\n",
       "      <td>0.503459</td>\n",
       "      <td>0.094612</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.251573</td>\n",
       "      <td>-0.069744</td>\n",
       "      <td>-0.254906</td>\n",
       "      <td>0.340350</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>-0.210544</td>\n",
       "      <td>0.329385</td>\n",
       "      <td>0.327490</td>\n",
       "      <td>0.094869</td>\n",
       "      <td>0.076083</td>\n",
       "      <td>0.379889</td>\n",
       "      <td>0.252079</td>\n",
       "      <td>-0.072443</td>\n",
       "      <td>0.392215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.077278</td>\n",
       "      <td>-0.206370</td>\n",
       "      <td>-0.047961</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>-0.161974</td>\n",
       "      <td>-0.012756</td>\n",
       "      <td>0.216498</td>\n",
       "      <td>0.301206</td>\n",
       "      <td>-0.242728</td>\n",
       "      <td>0.298649</td>\n",
       "      <td>0.187011</td>\n",
       "      <td>0.092538</td>\n",
       "      <td>0.062393</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.057514</td>\n",
       "      <td>0.050180</td>\n",
       "      <td>0.011214</td>\n",
       "      <td>0.142624</td>\n",
       "      <td>0.067230</td>\n",
       "      <td>0.149156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.128416</td>\n",
       "      <td>-0.210681</td>\n",
       "      <td>-0.384196</td>\n",
       "      <td>0.558968</td>\n",
       "      <td>0.308512</td>\n",
       "      <td>0.088604</td>\n",
       "      <td>0.323941</td>\n",
       "      <td>0.385053</td>\n",
       "      <td>0.093072</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.265408</td>\n",
       "      <td>0.340614</td>\n",
       "      <td>-0.101602</td>\n",
       "      <td>0.120984</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.298259</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.448566</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>0.125254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>0.074855</td>\n",
       "      <td>0.406301</td>\n",
       "      <td>-0.017198</td>\n",
       "      <td>-0.036295</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>0.112934</td>\n",
       "      <td>-0.297260</td>\n",
       "      <td>0.484979</td>\n",
       "      <td>0.065313</td>\n",
       "      <td>-0.033445</td>\n",
       "      <td>0.138405</td>\n",
       "      <td>0.522675</td>\n",
       "      <td>0.219032</td>\n",
       "      <td>-0.061316</td>\n",
       "      <td>0.238501</td>\n",
       "      <td>0.347193</td>\n",
       "      <td>-0.170549</td>\n",
       "      <td>-0.061903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.388044</td>\n",
       "      <td>-0.356181</td>\n",
       "      <td>-0.018401</td>\n",
       "      <td>-0.128434</td>\n",
       "      <td>-0.317250</td>\n",
       "      <td>0.045254</td>\n",
       "      <td>0.395414</td>\n",
       "      <td>0.360145</td>\n",
       "      <td>-0.452670</td>\n",
       "      <td>0.264799</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.125887</td>\n",
       "      <td>0.240156</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>-0.045951</td>\n",
       "      <td>0.168431</td>\n",
       "      <td>-0.019407</td>\n",
       "      <td>0.392041</td>\n",
       "      <td>0.150402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.630400</td>\n",
       "      <td>0.501246</td>\n",
       "      <td>-0.224306</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>-0.012412</td>\n",
       "      <td>0.153892</td>\n",
       "      <td>0.206282</td>\n",
       "      <td>-0.446852</td>\n",
       "      <td>0.597338</td>\n",
       "      <td>-0.069591</td>\n",
       "      <td>-0.049975</td>\n",
       "      <td>0.034849</td>\n",
       "      <td>0.414239</td>\n",
       "      <td>0.277493</td>\n",
       "      <td>0.221437</td>\n",
       "      <td>0.175239</td>\n",
       "      <td>0.392602</td>\n",
       "      <td>0.199786</td>\n",
       "      <td>0.523683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.259008</td>\n",
       "      <td>0.035282</td>\n",
       "      <td>0.109682</td>\n",
       "      <td>0.182587</td>\n",
       "      <td>0.126378</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>0.125440</td>\n",
       "      <td>0.167226</td>\n",
       "      <td>-0.135784</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.085053</td>\n",
       "      <td>0.069433</td>\n",
       "      <td>0.058312</td>\n",
       "      <td>0.223175</td>\n",
       "      <td>0.155352</td>\n",
       "      <td>-0.097075</td>\n",
       "      <td>0.108383</td>\n",
       "      <td>0.129631</td>\n",
       "      <td>0.035732</td>\n",
       "      <td>0.361397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.274998</td>\n",
       "      <td>-0.158230</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>0.545048</td>\n",
       "      <td>0.106536</td>\n",
       "      <td>0.101640</td>\n",
       "      <td>0.125629</td>\n",
       "      <td>0.242978</td>\n",
       "      <td>-0.285608</td>\n",
       "      <td>0.335965</td>\n",
       "      <td>0.148325</td>\n",
       "      <td>-0.055277</td>\n",
       "      <td>0.123817</td>\n",
       "      <td>0.394475</td>\n",
       "      <td>0.040476</td>\n",
       "      <td>-0.145576</td>\n",
       "      <td>0.191249</td>\n",
       "      <td>0.182777</td>\n",
       "      <td>-0.149076</td>\n",
       "      <td>0.443615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.219444</td>\n",
       "      <td>0.089313</td>\n",
       "      <td>-0.119144</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>-0.000555</td>\n",
       "      <td>-0.118553</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.028780</td>\n",
       "      <td>0.200502</td>\n",
       "      <td>-0.024381</td>\n",
       "      <td>0.026699</td>\n",
       "      <td>0.132393</td>\n",
       "      <td>0.165613</td>\n",
       "      <td>0.169675</td>\n",
       "      <td>0.118167</td>\n",
       "      <td>-0.269737</td>\n",
       "      <td>0.175712</td>\n",
       "      <td>0.462878</td>\n",
       "      <td>-0.039664</td>\n",
       "      <td>0.317488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.117179</td>\n",
       "      <td>-0.073265</td>\n",
       "      <td>-0.088605</td>\n",
       "      <td>0.204296</td>\n",
       "      <td>0.279813</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.384489</td>\n",
       "      <td>0.249603</td>\n",
       "      <td>-0.135611</td>\n",
       "      <td>-0.128529</td>\n",
       "      <td>0.092906</td>\n",
       "      <td>0.177479</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.184397</td>\n",
       "      <td>-0.051128</td>\n",
       "      <td>-0.125672</td>\n",
       "      <td>0.205984</td>\n",
       "      <td>-0.045163</td>\n",
       "      <td>-0.152840</td>\n",
       "      <td>0.076716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2000      2001      2002      2003      2004      2005      2006  \\\n",
       "0   0.006869 -0.339676  0.000563  0.359279  0.171095  0.047010  0.185383   \n",
       "1   0.002751  0.163384 -0.099845  0.851785  0.195363  0.224479  0.076721   \n",
       "2   0.000113 -0.248466 -0.209897  0.401704  0.205888 -0.128790  0.435979   \n",
       "3  -0.331899  0.120603 -0.526192  0.478752  0.205863 -0.049030  0.012380   \n",
       "4  -0.240567  0.438470 -0.352779  0.185081  0.069837 -0.161632  0.194059   \n",
       "5  -0.277085  0.027439 -0.510107  1.008141 -0.272635  0.069775 -0.179676   \n",
       "6   0.141905  0.151746 -0.075660 -0.030887  0.251665 -0.032509  0.106585   \n",
       "7  -0.044133 -0.158802 -0.316161  0.503459  0.094612  0.044831  0.251573   \n",
       "8   0.077278 -0.206370 -0.047961  0.172817 -0.161974 -0.012756  0.216498   \n",
       "9  -0.128416 -0.210681 -0.384196  0.558968  0.308512  0.088604  0.323941   \n",
       "10  0.286055  0.021827  0.074855  0.406301 -0.017198 -0.036295  0.026380   \n",
       "11  0.388044 -0.356181 -0.018401 -0.128434 -0.317250  0.045254  0.395414   \n",
       "12 -0.630400  0.501246 -0.224306  0.055847  0.080927 -0.012412  0.153892   \n",
       "13 -0.259008  0.035282  0.109682  0.182587  0.126378  0.060791  0.125440   \n",
       "14  0.274998 -0.158230 -0.028285  0.545048  0.106536  0.101640  0.125629   \n",
       "15 -0.219444  0.089313 -0.119144  0.054353 -0.000555 -0.118553  0.010832   \n",
       "16  0.117179 -0.073265 -0.088605  0.204296  0.279813  0.122900  0.384489   \n",
       "\n",
       "        2007      2008      2009      2010      2011      2012      2013  \\\n",
       "0  -0.143251 -0.638984  1.258279  0.070672  0.106434  0.206819  0.569791   \n",
       "1   0.197358 -0.368758  0.324359  0.666731 -0.022303 -0.007904 -0.002906   \n",
       "2  -0.046332 -0.286749  0.432804  0.175764  0.010597  0.331345  0.522512   \n",
       "3  -0.330379 -0.116666  0.301407  0.240235  0.229130  0.490326  0.322462   \n",
       "4   0.128171 -0.214115  0.591181  0.140393  0.270309  0.043179 -0.014898   \n",
       "5   0.329087 -0.426530  0.436096  0.042641  0.194452 -0.133559  0.275995   \n",
       "6   0.034375 -0.075747  0.107210 -0.010431  0.085230  0.108091  0.328713   \n",
       "7  -0.069744 -0.254906  0.340350  0.018913 -0.210544  0.329385  0.327490   \n",
       "8   0.301206 -0.242728  0.298649  0.187011  0.092538  0.062393  0.148889   \n",
       "9   0.385053  0.093072  0.036914  0.265408  0.340614 -0.101602  0.120984   \n",
       "10  0.112934 -0.297260  0.484979  0.065313 -0.033445  0.138405  0.522675   \n",
       "11  0.360145 -0.452670  0.264799  0.020802  0.087838  0.125887  0.240156   \n",
       "12  0.206282 -0.446852  0.597338 -0.069591 -0.049975  0.034849  0.414239   \n",
       "13  0.167226 -0.135784  0.014134  0.085053  0.069433  0.058312  0.223175   \n",
       "14  0.242978 -0.285608  0.335965  0.148325 -0.055277  0.123817  0.394475   \n",
       "15  0.028780  0.200502 -0.024381  0.026699  0.132393  0.165613  0.169675   \n",
       "16  0.249603 -0.135611 -0.128529  0.092906  0.177479  0.032476  0.184397   \n",
       "\n",
       "        2014      2015      2016      2017      2018      2019  \n",
       "0   0.034735 -0.243239  0.108990  0.347316 -0.030422  0.304835  \n",
       "1   0.039185 -0.231932  0.444654  0.726567 -0.179165  0.204403  \n",
       "2   0.254216  0.120639  0.025639  0.036984  0.022466  0.346565  \n",
       "3   0.306183  0.283122  0.052467  0.435203 -0.076475  0.427053  \n",
       "4  -0.122261 -0.118897  0.270549 -0.045640 -0.230799  0.257397  \n",
       "5   0.452810 -0.032263  0.106119  0.296264  0.037185  0.292457  \n",
       "6   0.179146  0.006958  0.164676  0.237922 -0.050951  0.075296  \n",
       "7   0.094869  0.076083  0.379889  0.252079 -0.072443  0.392215  \n",
       "8   0.057514  0.050180  0.011214  0.142624  0.067230  0.149156  \n",
       "9   0.001155  0.298259  0.070100  0.448566  0.048030  0.125254  \n",
       "10  0.219032 -0.061316  0.238501  0.347193 -0.170549 -0.061903  \n",
       "11  0.173469 -0.045951  0.168431 -0.019407  0.392041  0.150402  \n",
       "12  0.277493  0.221437  0.175239  0.392602  0.199786  0.523683  \n",
       "13  0.155352 -0.097075  0.108383  0.129631  0.035732  0.361397  \n",
       "14  0.040476 -0.145576  0.191249  0.182777 -0.149076  0.443615  \n",
       "15  0.118167 -0.269737  0.175712  0.462878 -0.039664  0.317488  \n",
       "16 -0.051128 -0.125672  0.205984 -0.045163 -0.152840  0.076716  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns not needed\n",
    "\n",
    "performance = performance.drop(['otu_id', 'otu_label'], axis=1)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.331899</td>\n",
       "      <td>-0.240567</td>\n",
       "      <td>-0.277085</td>\n",
       "      <td>0.141905</td>\n",
       "      <td>-0.044133</td>\n",
       "      <td>0.077278</td>\n",
       "      <td>-0.128416</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.388044</td>\n",
       "      <td>-0.630400</td>\n",
       "      <td>-0.259008</td>\n",
       "      <td>0.274998</td>\n",
       "      <td>-0.219444</td>\n",
       "      <td>0.117179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.339676</td>\n",
       "      <td>0.163384</td>\n",
       "      <td>-0.248466</td>\n",
       "      <td>0.120603</td>\n",
       "      <td>0.438470</td>\n",
       "      <td>0.027439</td>\n",
       "      <td>0.151746</td>\n",
       "      <td>-0.158802</td>\n",
       "      <td>-0.206370</td>\n",
       "      <td>-0.210681</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>-0.356181</td>\n",
       "      <td>0.501246</td>\n",
       "      <td>0.035282</td>\n",
       "      <td>-0.158230</td>\n",
       "      <td>0.089313</td>\n",
       "      <td>-0.073265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>0.000563</td>\n",
       "      <td>-0.099845</td>\n",
       "      <td>-0.209897</td>\n",
       "      <td>-0.526192</td>\n",
       "      <td>-0.352779</td>\n",
       "      <td>-0.510107</td>\n",
       "      <td>-0.075660</td>\n",
       "      <td>-0.316161</td>\n",
       "      <td>-0.047961</td>\n",
       "      <td>-0.384196</td>\n",
       "      <td>0.074855</td>\n",
       "      <td>-0.018401</td>\n",
       "      <td>-0.224306</td>\n",
       "      <td>0.109682</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>-0.119144</td>\n",
       "      <td>-0.088605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.359279</td>\n",
       "      <td>0.851785</td>\n",
       "      <td>0.401704</td>\n",
       "      <td>0.478752</td>\n",
       "      <td>0.185081</td>\n",
       "      <td>1.008141</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>0.503459</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>0.558968</td>\n",
       "      <td>0.406301</td>\n",
       "      <td>-0.128434</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.182587</td>\n",
       "      <td>0.545048</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.204296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.171095</td>\n",
       "      <td>0.195363</td>\n",
       "      <td>0.205888</td>\n",
       "      <td>0.205863</td>\n",
       "      <td>0.069837</td>\n",
       "      <td>-0.272635</td>\n",
       "      <td>0.251665</td>\n",
       "      <td>0.094612</td>\n",
       "      <td>-0.161974</td>\n",
       "      <td>0.308512</td>\n",
       "      <td>-0.017198</td>\n",
       "      <td>-0.317250</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>0.126378</td>\n",
       "      <td>0.106536</td>\n",
       "      <td>-0.000555</td>\n",
       "      <td>0.279813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "2000  0.006869  0.002751  0.000113 -0.331899 -0.240567 -0.277085  0.141905   \n",
       "2001 -0.339676  0.163384 -0.248466  0.120603  0.438470  0.027439  0.151746   \n",
       "2002  0.000563 -0.099845 -0.209897 -0.526192 -0.352779 -0.510107 -0.075660   \n",
       "2003  0.359279  0.851785  0.401704  0.478752  0.185081  1.008141 -0.030887   \n",
       "2004  0.171095  0.195363  0.205888  0.205863  0.069837 -0.272635  0.251665   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "2000 -0.044133  0.077278 -0.128416  0.286055  0.388044 -0.630400 -0.259008   \n",
       "2001 -0.158802 -0.206370 -0.210681  0.021827 -0.356181  0.501246  0.035282   \n",
       "2002 -0.316161 -0.047961 -0.384196  0.074855 -0.018401 -0.224306  0.109682   \n",
       "2003  0.503459  0.172817  0.558968  0.406301 -0.128434  0.055847  0.182587   \n",
       "2004  0.094612 -0.161974  0.308512 -0.017198 -0.317250  0.080927  0.126378   \n",
       "\n",
       "            14        15        16  \n",
       "2000  0.274998 -0.219444  0.117179  \n",
       "2001 -0.158230  0.089313 -0.073265  \n",
       "2002 -0.028285 -0.119144 -0.088605  \n",
       "2003  0.545048  0.054353  0.204296  \n",
       "2004  0.106536 -0.000555  0.279813  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = (performance.T)\n",
    "performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ~ n_samples x n_features: (20, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.86869800e-03,  2.75084000e-03,  1.13338000e-04,\n",
       "        -3.31899206e-01, -2.40566854e-01, -2.77084859e-01,\n",
       "         1.41905316e-01, -4.41331850e-02,  7.72783430e-02,\n",
       "        -1.28415672e-01,  2.86055457e-01,  3.88043908e-01,\n",
       "        -6.30399591e-01, -2.59007921e-01,  2.74997949e-01,\n",
       "        -2.19443633e-01,  1.17179269e-01],\n",
       "       [-3.39675651e-01,  1.63384060e-01, -2.48466068e-01,\n",
       "         1.20602887e-01,  4.38470238e-01,  2.74385300e-02,\n",
       "         1.51746217e-01, -1.58802277e-01, -2.06370168e-01,\n",
       "        -2.10680621e-01,  2.18268490e-02, -3.56181240e-01,\n",
       "         5.01246318e-01,  3.52817220e-02, -1.58229587e-01,\n",
       "         8.93126860e-02, -7.32651540e-02],\n",
       "       [ 5.62783000e-04, -9.98454840e-02, -2.09897155e-01,\n",
       "        -5.26191532e-01, -3.52779026e-01, -5.10106754e-01,\n",
       "        -7.56595500e-02, -3.16161096e-01, -4.79609900e-02,\n",
       "        -3.84195762e-01,  7.48552940e-02, -1.84006170e-02,\n",
       "        -2.24306077e-01,  1.09681510e-01, -2.82852760e-02,\n",
       "        -1.19144053e-01, -8.86051930e-02],\n",
       "       [ 3.59278639e-01,  8.51784685e-01,  4.01703640e-01,\n",
       "         4.78751548e-01,  1.85080618e-01,  1.00814097e+00,\n",
       "        -3.08868320e-02,  5.03459425e-01,  1.72816899e-01,\n",
       "         5.58967600e-01,  4.06300849e-01, -1.28433643e-01,\n",
       "         5.58469100e-02,  1.82586992e-01,  5.45047695e-01,\n",
       "         5.43529750e-02,  2.04296282e-01],\n",
       "       [ 1.71094710e-01,  1.95362758e-01,  2.05887999e-01,\n",
       "         2.05863177e-01,  6.98368340e-02, -2.72635206e-01,\n",
       "         2.51665224e-01,  9.46118580e-02, -1.61973627e-01,\n",
       "         3.08512027e-01, -1.71976060e-02, -3.17250222e-01,\n",
       "         8.09266030e-02,  1.26378113e-01,  1.06536210e-01,\n",
       "        -5.55471000e-04,  2.79812694e-01],\n",
       "       [ 4.70102270e-02,  2.24478709e-01, -1.28789670e-01,\n",
       "        -4.90304030e-02, -1.61632187e-01,  6.97751570e-02,\n",
       "        -3.25091640e-02,  4.48306980e-02, -1.27558170e-02,\n",
       "         8.86037860e-02, -3.62949000e-02,  4.52542220e-02,\n",
       "        -1.24116840e-02,  6.07908990e-02,  1.01640135e-01,\n",
       "        -1.18553298e-01,  1.22900368e-01],\n",
       "       [ 1.85383418e-01,  7.67210780e-02,  4.35979247e-01,\n",
       "         1.23798670e-02,  1.94059036e-01, -1.79676065e-01,\n",
       "         1.06585179e-01,  2.51572791e-01,  2.16498219e-01,\n",
       "         3.23940958e-01,  2.63803090e-02,  3.95414137e-01,\n",
       "         1.53892436e-01,  1.25440105e-01,  1.25628879e-01,\n",
       "         1.08322680e-02,  3.84488628e-01],\n",
       "       [-1.43251065e-01,  1.97357572e-01, -4.63321510e-02,\n",
       "        -3.30378576e-01,  1.28171165e-01,  3.29087274e-01,\n",
       "         3.43750540e-02, -6.97442460e-02,  3.01206391e-01,\n",
       "         3.85052650e-01,  1.12933918e-01,  3.60144972e-01,\n",
       "         2.06282173e-01,  1.67225839e-01,  2.42977686e-01,\n",
       "         2.87796560e-02,  2.49603226e-01],\n",
       "       [-6.38984437e-01, -3.68757669e-01, -2.86749036e-01,\n",
       "        -1.16666104e-01, -2.14114611e-01, -4.26529670e-01,\n",
       "        -7.57465180e-02, -2.54906245e-01, -2.42728101e-01,\n",
       "         9.30723260e-02, -2.97260135e-01, -4.52670081e-01,\n",
       "        -4.46851739e-01, -1.35784415e-01, -2.85608461e-01,\n",
       "         2.00501777e-01, -1.35611055e-01],\n",
       "       [ 1.25827901e+00,  3.24358567e-01,  4.32804174e-01,\n",
       "         3.01407031e-01,  5.91180963e-01,  4.36095983e-01,\n",
       "         1.07209803e-01,  3.40349684e-01,  2.98648842e-01,\n",
       "         3.69137380e-02,  4.84978879e-01,  2.64798510e-01,\n",
       "         5.97338147e-01,  1.41341600e-02,  3.35965397e-01,\n",
       "        -2.43809250e-02, -1.28528869e-01],\n",
       "       [ 7.06717210e-02,  6.66731456e-01,  1.75763821e-01,\n",
       "         2.40235023e-01,  1.40392738e-01,  4.26414430e-02,\n",
       "        -1.04311340e-02,  1.89133020e-02,  1.87011209e-01,\n",
       "         2.65407586e-01,  6.53134670e-02,  2.08024890e-02,\n",
       "        -6.95905180e-02,  8.50526190e-02,  1.48324591e-01,\n",
       "         2.66993830e-02,  9.29056460e-02],\n",
       "       [ 1.06434221e-01, -2.23027170e-02,  1.05970250e-02,\n",
       "         2.29129549e-01,  2.70309139e-01,  1.94451920e-01,\n",
       "         8.52297660e-02, -2.10543893e-01,  9.25379490e-02,\n",
       "         3.40613654e-01, -3.34454080e-02,  8.78383590e-02,\n",
       "        -4.99750900e-02,  6.94333610e-02, -5.52769250e-02,\n",
       "         1.32392668e-01,  1.77478850e-01],\n",
       "       [ 2.06819442e-01, -7.90363300e-03,  3.31344741e-01,\n",
       "         4.90326472e-01,  4.31790580e-02, -1.33559330e-01,\n",
       "         1.08090783e-01,  3.29384589e-01,  6.23925830e-02,\n",
       "        -1.01602251e-01,  1.38405119e-01,  1.25887034e-01,\n",
       "         3.48493560e-02,  5.83119770e-02,  1.23816985e-01,\n",
       "         1.65612977e-01,  3.24758440e-02],\n",
       "       [ 5.69791137e-01, -2.90625200e-03,  5.22512131e-01,\n",
       "         3.22462140e-01, -1.48975950e-02,  2.75994799e-01,\n",
       "         3.28713145e-01,  3.27489614e-01,  1.48889110e-01,\n",
       "         1.20984269e-01,  5.22675240e-01,  2.40155799e-01,\n",
       "         4.14238734e-01,  2.23174540e-01,  3.94474529e-01,\n",
       "         1.69675337e-01,  1.84397440e-01],\n",
       "       [ 3.47352560e-02,  3.91854970e-02,  2.54215980e-01,\n",
       "         3.06182764e-01, -1.22260606e-01,  4.52810169e-01,\n",
       "         1.79146175e-01,  9.48689500e-02,  5.75140740e-02,\n",
       "         1.15527400e-03,  2.19032330e-01,  1.73468644e-01,\n",
       "         2.77493265e-01,  1.55352328e-01,  4.04762700e-02,\n",
       "         1.18166818e-01, -5.11282580e-02],\n",
       "       [-2.43238561e-01, -2.31932335e-01,  1.20638901e-01,\n",
       "         2.83121848e-01, -1.18896699e-01, -3.22633240e-02,\n",
       "         6.95788500e-03,  7.60826460e-02,  5.01797960e-02,\n",
       "         2.98259050e-01, -6.13159490e-02, -4.59512780e-02,\n",
       "         2.21436569e-01, -9.70750290e-02, -1.45575607e-01,\n",
       "        -2.69737406e-01, -1.25672448e-01],\n",
       "       [ 1.08989693e-01,  4.44654294e-01,  2.56393370e-02,\n",
       "         5.24670560e-02,  2.70548576e-01,  1.06118867e-01,\n",
       "         1.64675600e-01,  3.79889486e-01,  1.12135140e-02,\n",
       "         7.00995640e-02,  2.38500909e-01,  1.68431270e-01,\n",
       "         1.75239424e-01,  1.08382941e-01,  1.91249428e-01,\n",
       "         1.75712266e-01,  2.05983872e-01],\n",
       "       [ 3.47316212e-01,  7.26567387e-01,  3.69835630e-02,\n",
       "         4.35203127e-01, -4.56400460e-02,  2.96264157e-01,\n",
       "         2.37921614e-01,  2.52078734e-01,  1.42624047e-01,\n",
       "         4.48566362e-01,  3.47193237e-01, -1.94068480e-02,\n",
       "         3.92602223e-01,  1.29631199e-01,  1.82776873e-01,\n",
       "         4.62877827e-01, -4.51628880e-02],\n",
       "       [-3.04222030e-02, -1.79165097e-01,  2.24657130e-02,\n",
       "        -7.64750440e-02, -2.30798769e-01,  3.71850680e-02,\n",
       "        -5.09505390e-02, -7.24426750e-02,  6.72299450e-02,\n",
       "         4.80296900e-02, -1.70549254e-01,  3.92040526e-01,\n",
       "         1.99785715e-01,  3.57318110e-02, -1.49076248e-01,\n",
       "        -3.96635230e-02, -1.52839729e-01],\n",
       "       [ 3.04834558e-01,  2.04403421e-01,  3.46565350e-01,\n",
       "         4.27053110e-01,  2.57396742e-01,  2.92456726e-01,\n",
       "         7.52956730e-02,  3.92214574e-01,  1.49156416e-01,\n",
       "         1.25253912e-01, -6.19028960e-02,  1.50401781e-01,\n",
       "         5.23682555e-01,  3.61397455e-01,  4.43614694e-01,\n",
       "         3.17487504e-01,  7.67155350e-02]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = performance\n",
    "print('X ~ n_samples x n_features:', X.shape)\n",
    "#type(X)\n",
    "\n",
    "#X = list(X.values)\n",
    "X = X.to_numpy()\n",
    "X\n",
    "\n",
    "\n",
    "# li1 = []\n",
    "# for index, row in X.iterrows():\n",
    "#      #print(list(row))\n",
    "#      li1.append([row])\n",
    "# li1 = list(X.values)\n",
    "# X = li1\n",
    "# li1 = np.array(li1)\n",
    "# type (li1)\n",
    "# li1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the csv file into a pandas DataFrame\n",
    "\n",
    "# potus = pd.read_csv('../data/potus.csv')\n",
    "# potus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reformat data\n",
    "# data = potus.values\n",
    "# y = data[:, 1]\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Label-encode data set\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(y)\n",
    "# encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, original_class in zip(encoded_y, y):\n",
    "#     print('Original Class: ' + str(original_class))\n",
    "#     print('Encoded Label: ' + str(label))\n",
    "#     print('-' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "# # One-hot encoding\n",
    "# one_hot_y = to_categorical(encoded_y)\n",
    "# one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>potus</th>\n",
       "      <th>one_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       potus  one_hot\n",
       "0  2000    Democrat        0\n",
       "1  2001  Republican        1\n",
       "2  2002  Republican        1\n",
       "3  2003  Republican        1\n",
       "4  2004  Republican        1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file into a pandas DataFrame\n",
    "\n",
    "potus_one_hot = pd.read_csv('../data/potus_one_hot.csv')\n",
    "potus_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat data\n",
    "data = potus_one_hot.values\n",
    "y = data[:, 2]\n",
    "y=y.astype('int')\n",
    "y.dtype\n",
    "# y = [[int(i)] for i in y]\n",
    "# type(y)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y ~ n_samples: (20,)\n"
     ]
    }
   ],
   "source": [
    "print('y ~ n_samples:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a supervised task, and since we are interested in its performance on unseen data, we split our data into two parts:\n",
    "\n",
    "1. a training set that the learning algorithm uses to fit the model\n",
    "2. a test set to evaluate the generalization performance of the model\n",
    "\n",
    "The ``train_test_split`` function from the ``model_selection`` module does that for us -- we will use it to split a dataset into 75% training data and 25% test data.\n",
    "\n",
    "<img src=\"figures/train_test_split_matrix.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train.reshape(-1, 1))\n",
    "#X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler and y_scaler models\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape\n",
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1, Train/Test Score: 1.000/0.800\n",
      "k: 3, Train/Test Score: 0.667/0.800\n",
      "k: 5, Train/Test Score: 0.467/0.600\n",
      "k: 7, Train/Test Score: 0.467/0.600\n",
      "k: 9, Train/Test Score: 0.733/0.600\n",
      "k: 11, Train/Test Score: 0.533/0.400\n",
      "k: 13, Train/Test Score: 0.600/0.600\n",
      "k: 15, Train/Test Score: 0.533/0.600\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 15, n_neighbors = 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-b31c60b7b83b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#    knn.fit(X_train, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#     train_score = knn.score(X_train, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             )\n\u001b[0;32m    418\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 15, n_neighbors = 17"
     ]
    }
   ],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 14, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#    knn.fit(X_train, y_train)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "#     train_score = knn.score(X_train, y_train)\n",
    "#     test_score = knn.score(X_test, y_test)\n",
    "#     train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k: 9 provides the best accuracy where the classifier starts to stablize\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('k=9 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.30695783 -0.73511651 -2.42873363  1.72909751  1.59456586]\n",
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print(prediction)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(prediction == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a convenience function , ``score``, that all scikit-learn classifiers have to compute this directly from the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.419797305516562"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
